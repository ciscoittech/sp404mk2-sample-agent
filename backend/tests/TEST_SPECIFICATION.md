# Audio Features Service - Test Specification (TDD RED Phase)

**Status:** ✅ Tests Written - Ready for Implementation
**Phase:** RED (Tests Fail)
**Next Phase:** GREEN (Implement code to make tests pass)

---

## Overview

This document describes the 3 failing tests written for the Audio Features Service following TDD (Test-Driven Development) methodology. These tests define the expected behavior of the service before implementation.

### Test Strategy

- **MVP Level:** Exactly 3 tests (no more, no less)
- **Integration Testing:** 2 out of 3 tests are REAL_INTEGRATION_TEST
- **No Mocking:** Real librosa, real database, real file I/O
- **Async Support:** All tests use pytest-asyncio

---

## Test Files

### 1. Main Test File
**Location:** `backend/tests/test_audio_features_service.py`

Contains 3 comprehensive tests:
- `test_analyze_real_wav_file()` - REAL_INTEGRATION_TEST
- `test_invalid_file_raises_audio_error()` - Error handling
- `test_save_features_to_database()` - REAL_INTEGRATION_TEST

### 2. Fixtures (conftest.py additions)
**Location:** `backend/tests/conftest.py`

New fixtures added:
- `audio_service()` - Provides AudioFeaturesService instance
- `test_wav_fixture()` - Generates real 2-second WAV file (440Hz sine wave)

### 3. Supporting Files
- `generate_test_fixtures.py` - Manual fixture generation utility
- `validate_test_structure.py` - Test structure validator (pre-implementation check)

---

## Test Specifications

### Test 1: Real WAV Analysis (REAL_INTEGRATION_TEST)

**Test:** `test_analyze_real_wav_file`
**Type:** Integration Test (No Mocking)
**Purpose:** Validate real audio analysis with librosa

#### What It Tests
- Service can load and analyze real audio files
- Librosa integration works correctly
- Returns AudioFeatures with reasonable values
- At least 50% of features are successfully extracted

#### Expected Behavior
```python
features = await audio_service.analyze_file(test_wav_fixture)

# Returns AudioFeatures instance
assert isinstance(features, AudioFeatures)

# Duration validation (~2 seconds)
assert 1.5 <= features.duration_seconds <= 2.5

# BPM validation (if detected)
if features.bpm is not None:
    assert 40 <= features.bpm <= 200

# Key validation (if detected)
valid_keys = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
if features.key is not None:
    assert features.key in valid_keys

# Spectral features validation (if detected)
if features.spectral_centroid is not None:
    assert features.spectral_centroid > 0

# Feature completeness: At least 50% non-None
assert completeness_ratio >= 0.5
```

#### Real Audio Fixture
- **Format:** WAV (44.1kHz, mono)
- **Duration:** 2 seconds
- **Content:** 440Hz sine wave (A4 note)
- **Generated by:** `test_wav_fixture()` pytest fixture using numpy + soundfile/scipy

#### Why No Mocking?
This is a REAL_INTEGRATION_TEST. We need to verify that:
1. Librosa can actually load and analyze WAV files
2. The service handles real audio data correctly
3. Feature extraction produces sensible values
4. The full pipeline works end-to-end

---

### Test 2: Error Handling

**Test:** `test_invalid_file_raises_audio_error`
**Type:** Error Validation (No Mocking)
**Purpose:** Validate proper error handling for invalid inputs

#### What It Tests
- Non-existent files raise AudioError
- Corrupted files raise AudioError
- Empty files raise AudioError
- AudioError includes proper attributes (message, file_path, original_error)

#### Expected Behavior
```python
# Test 1: Non-existent file
with pytest.raises(AudioError) as exc_info:
    await audio_service.analyze_file(non_existent_path)

error = exc_info.value
assert error.file_path == non_existent_path
assert "not found" in error.message.lower()

# Test 2: Corrupted file
with pytest.raises(AudioError) as exc_info:
    await audio_service.analyze_file(corrupted_path)

error = exc_info.value
assert error.file_path == corrupted_path
assert error.original_error is not None

# Test 3: Empty file
with pytest.raises(AudioError) as exc_info:
    await audio_service.analyze_file(empty_path)
```

#### Error Structure
```python
class AudioError(Exception):
    def __init__(self, message: str, file_path: Path, original_error: Exception = None):
        self.message = message
        self.file_path = file_path
        self.original_error = original_error
```

---

### Test 3: Database Integration (REAL_INTEGRATION_TEST)

**Test:** `test_save_features_to_database`
**Type:** Integration Test (No Mocking)
**Purpose:** Validate round-trip through database

#### What It Tests
- Features can be serialized to JSON
- Sample.extra_metadata JSONB field stores features correctly
- Features can be deserialized after retrieval
- All feature values are preserved through serialization

#### Expected Behavior
```python
# Analyze real WAV
features = await audio_service.analyze_file(test_wav_fixture)

# Save to database
sample = Sample(
    user_id=test_user.id,
    title="Test Audio Features Sample",
    file_path=str(test_wav_fixture),
    extra_metadata={
        "audio_features": features.to_dict()
    }
)
db_session.add(sample)
await db_session.commit()

# Query back
retrieved_sample = await db_session.execute(...)
features_dict = retrieved_sample.extra_metadata["audio_features"]
retrieved_features = AudioFeatures.from_dict(features_dict)

# Verify all values preserved
assert retrieved_features.bpm == features.bpm
assert retrieved_features.key == features.key
assert retrieved_features.spectral_centroid == features.spectral_centroid
# ... etc
```

#### Database Setup
- **Database:** SQLite in-memory (from existing test infrastructure)
- **Session:** Real async SQLAlchemy session (not mocked)
- **Model:** Real Sample model with extra_metadata JSONB field
- **Serialization:** AudioFeatures.to_dict() / from_dict() methods

#### Why No Mocking?
This is a REAL_INTEGRATION_TEST. We need to verify:
1. JSONB serialization/deserialization works correctly
2. Sample model stores and retrieves features properly
3. No data loss through database round-trip
4. SQLAlchemy async operations work correctly

---

## Running the Tests

### Prerequisites

```bash
# Install dependencies (if needed)
pip install pytest pytest-asyncio numpy scipy soundfile librosa

# Fix existing SQLAlchemy issue (youtube.py metadata column conflict)
# This is a known issue in the codebase, not related to our tests
```

### Run Tests (After Implementation)

```bash
# From project root
cd backend

# Run all audio feature tests
../venv/bin/python -m pytest tests/test_audio_features_service.py -v

# Run with coverage
../venv/bin/python -m pytest tests/test_audio_features_service.py --cov=app.services.audio_features_service --cov-report=html

# Run single test
../venv/bin/python -m pytest tests/test_audio_features_service.py::test_analyze_real_wav_file -v
```

### Validate Test Structure (Before Implementation)

```bash
# Validate tests are properly structured
cd backend
../venv/bin/python tests/validate_test_structure.py
```

---

## Implementation Requirements

The Coder agent must implement the following to make tests pass:

### 1. Models (`backend/app/models/audio_features.py`)

```python
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

@dataclass
class AudioFeatures:
    """Audio analysis results."""
    file_path: Path
    duration_seconds: float
    bpm: Optional[float] = None
    key: Optional[str] = None
    spectral_centroid: Optional[float] = None
    spectral_rolloff: Optional[float] = None
    zero_crossing_rate: Optional[float] = None

    def to_dict(self) -> dict:
        """Serialize to dict for JSON storage."""
        pass

    @classmethod
    def from_dict(cls, data: dict) -> "AudioFeatures":
        """Deserialize from dict."""
        pass


class AudioError(Exception):
    """Exception for audio processing errors."""
    def __init__(self, message: str, file_path: Path, original_error: Exception = None):
        self.message = message
        self.file_path = file_path
        self.original_error = original_error
        super().__init__(message)
```

### 2. Service (`backend/app/services/audio_features_service.py`)

```python
from pathlib import Path
from app.models.audio_features import AudioFeatures, AudioError

class AudioFeaturesService:
    """Service for analyzing audio files."""

    async def analyze_file(self, file_path: Path) -> AudioFeatures:
        """
        Analyze audio file and extract features.

        Args:
            file_path: Path to audio file

        Returns:
            AudioFeatures with analysis results

        Raises:
            AudioError: If file cannot be analyzed
        """
        pass  # Implementation goes here
```

### 3. Protocol (`backend/app/services/protocols/audio_features.py`)

```python
from typing import Protocol
from pathlib import Path
from app.models.audio_features import AudioFeatures

class AudioFeaturesProtocol(Protocol):
    """Protocol for audio feature extraction services."""

    async def analyze_file(self, file_path: Path) -> AudioFeatures:
        """Analyze audio file and extract features."""
        ...
```

---

## Test Execution Timeline

### Current Status: RED Phase ✅
- ✅ Tests written
- ✅ Fixtures created
- ✅ Structure validated
- ❌ Tests FAIL (service not implemented)

### Next: GREEN Phase
1. Implement AudioFeatures and AudioError models
2. Implement AudioFeaturesService with librosa
3. Run tests - should PASS
4. Verify 100% test coverage

### Finally: REFACTOR Phase
1. Optimize performance if needed
2. Improve error messages
3. Add logging
4. Tests continue to PASS

---

## Key Design Decisions

### Why Real Integration Tests?

**Decision:** Use real librosa and real database (no mocking)

**Rationale:**
1. **Confidence:** Proves the actual integration works, not just mocked behavior
2. **Realistic:** Tests real-world scenarios with actual audio files
3. **Fast Enough:** 2-second audio file analyzes in < 5 seconds
4. **MVP Level:** 3 comprehensive tests provide better coverage than 20 unit tests

### Why 3 Tests?

**Decision:** Exactly 3 tests, not 5 or 10

**Rationale:**
1. **MVP Principle:** Per project guidelines, 2-5 basic tests per feature
2. **Simplicity:** Avoids enterprise test bloat
3. **Coverage:** These 3 tests cover:
   - Happy path (real analysis)
   - Error handling (3 error scenarios)
   - Integration (database round-trip)

### Why Generate WAV Files?

**Decision:** Programmatically generate test audio instead of checking in binary files

**Rationale:**
1. **Git-Friendly:** No binary files in repo (except optional reference fixtures)
2. **Deterministic:** Same fixture every time, no surprises
3. **Flexible:** Easy to generate different fixtures for different tests
4. **Fast:** Generating 2-second WAV takes < 100ms

---

## Success Criteria

Tests will be considered successful when:

1. ✅ All 3 tests can be imported without errors
2. ✅ Fixtures generate valid WAV files
3. ❌ Tests FAIL with clear error messages (RED phase)
4. ⏳ After implementation, tests PASS (GREEN phase)
5. ⏳ Code coverage >= 80% for audio_features_service.py

---

## Notes for Coder Agent

### What to Implement

1. **Models First:** Start with AudioFeatures and AudioError
2. **Service Next:** Implement analyze_file() method
3. **Librosa Integration:** Use librosa for actual audio analysis
4. **Error Handling:** Catch and wrap all exceptions as AudioError

### What NOT to Implement

- ❌ Don't add more tests (we have exactly 3)
- ❌ Don't mock librosa or database
- ❌ Don't skip error handling
- ❌ Don't add complex features beyond the tests

### Performance Expectations

- Analysis should complete in < 10 seconds per file
- Memory usage should be reasonable (< 100MB for 2-second file)
- Should handle missing dependencies gracefully

---

## Validation Results

```
✅ Test file structure is valid
✅ All 3 required tests present
✅ 3 REAL_INTEGRATION_TEST markers found
✅ All required imports present
✅ Contains proper assertions
✅ All fixtures defined
✅ TDD RED PHASE COMPLETE
```

**Ready to hand off to Coder agent for implementation!**

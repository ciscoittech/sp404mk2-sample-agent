"""Collector Agent - AI-powered sample discovery and categorization."""

import os
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple
from pydantic_ai import Agent as PydanticAgent
from pydantic_ai.models import Model
from pydantic import BaseModel, Field

from ..logging_config import AgentLogger
from ..tools import database
from .base import Agent, AgentResult, AgentStatus


# Pydantic models for AI responses
class SearchQueries(BaseModel):
    """Search queries generated by AI."""
    queries: List[str] = Field(description="List of search queries")
    reasoning: str = Field(description="Reasoning behind query selection")


class CategorizedSource(BaseModel):
    """Categorized source with relevance."""
    url: str
    category: str
    relevance: float = Field(ge=0.0, le=1.0)
    reason: str


class QualityAssessment(BaseModel):
    """Quality assessment of a source."""
    url: str
    quality_score: float = Field(ge=0.0, le=1.0)
    factors: Dict[str, Any]


class CollectionSummary(BaseModel):
    """AI-generated collection summary."""
    description: str
    recommended_use: str
    key_characteristics: List[str]
    total_sources: int


# Import enhanced YouTube search
from ..tools.youtube_search import YouTubeSearcher

# Initialize YouTube searcher
youtube_searcher = YouTubeSearcher()


async def search_youtube(query: str, max_results: int = 10) -> List[Dict]:
    """Search YouTube for samples using enhanced search."""
    try:
        results = await youtube_searcher.search(query, max_results, filter_samples=True)
        # Convert to collector format
        return [
            {
                "url": result["url"],
                "title": result["title"],
                "duration": result["duration"],
                "views": result["view_count"],
                "platform": "youtube",
                "channel": result["channel"],
                "quality_score": result["quality_score"],
                "published_at": result["published_at"],
                "description": result["description"][:200] + "..." if len(result["description"]) > 200 else result["description"]
            }
            for result in results
        ]
    except Exception as e:
        # Fallback to mock if search fails
        return [
            {
                "url": f"https://youtube.com/watch?v={i}",
                "title": f"Sample {i} - {query}",
                "duration": 180 + i * 30,
                "views": 1000 * (i + 1),
                "platform": "youtube"
            }
            for i in range(min(max_results, 5))
        ]


async def search_soundcloud(query: str, max_results: int = 10) -> List[Dict]:
    """Search SoundCloud for samples."""
    return []


async def search_freesound(query: str, max_results: int = 10) -> List[Dict]:
    """Search Freesound for samples."""
    return []


def validate_url(url: str) -> bool:
    """Validate if URL is accessible."""
    # Simple validation for now
    return url.startswith(("http://", "https://"))


def estimate_download_size(url: str, duration: int) -> int:
    """Estimate download size based on duration."""
    # Rough estimate: 1MB per minute of audio
    return duration * 1_000_000 // 60


class CollectorAgent(Agent):
    """Agent responsible for discovering and categorizing samples using AI."""
    
    def __init__(self):
        """Initialize the Collector Agent with AI capabilities."""
        super().__init__("collector")
        self.logger = AgentLogger(self.name)
        
        # Initialize Pydantic AI agent with Google Flash 2.0
        # For now, we'll use a simple mock since we're focusing on the chat interface
        self.ai_agent = None  # Will implement proper OpenRouter integration later
        
    async def execute(self, task_id: str, **kwargs) -> AgentResult:
        """
        Discover and categorize samples based on criteria.
        
        Args:
            task_id: Unique task identifier
            genre: Target musical genre
            style: Specific style within genre
            bpm_range: Tuple of (min_bpm, max_bpm)
            era: Time period (e.g., "1990s")
            platforms: List of platforms to search
            max_results: Maximum results to return
            max_per_platform: Max results per platform
            quality_threshold: Minimum quality score
            sources: Pre-existing sources to categorize
            target_genre: Genre for categorization
            assess_quality: Whether to assess quality
            validate_urls: Whether to validate URLs
            estimate_sizes: Whether to estimate download sizes
            filter_criteria: Filtering criteria
            create_summary: Whether to create collection summary
            
        Returns:
            AgentResult with discovered/categorized sources
        """
        self.logger.set_task_id(task_id)
        self.logger.info("Starting collection task")
        started_at = datetime.now(timezone.utc)
        
        try:
            # Handle different operation modes
            if "sources" in kwargs and not any(k in kwargs for k in ["genre", "style"]):
                # Mode: Process existing sources
                return await self._process_existing_sources(task_id, started_at, **kwargs)
            else:
                # Mode: Discover new sources
                return await self._discover_new_sources(task_id, started_at, **kwargs)
                
        except Exception as e:
            self.logger.exception(f"Collection failed: {str(e)}")
            
            await database.add_agent_log({
                "task_id": task_id,
                "agent_type": self.name,
                "log_level": "error",
                "message": f"Collection failed: {str(e)}"
            })
            
            return AgentResult(
                agent_name=self.name,
                task_id=task_id,
                status=AgentStatus.FAILED,
                error=str(e),
                started_at=started_at,
                completed_at=datetime.now(timezone.utc)
            )
    
    async def _discover_new_sources(
        self, 
        task_id: str, 
        started_at: datetime,
        **kwargs
    ) -> AgentResult:
        """Discover new sources based on search criteria."""
        # Extract parameters
        genre = kwargs.get("genre", "")
        style = kwargs.get("style", "")
        bpm_range = kwargs.get("bpm_range")
        era = kwargs.get("era", "")
        platforms = kwargs.get("platforms", ["youtube"])
        max_results = kwargs.get("max_results", 10)
        max_per_platform = kwargs.get("max_per_platform", 10)
        quality_threshold = kwargs.get("quality_threshold", 0.0)
        
        # Generate search queries using AI
        queries = await self._generate_search_queries(
            genre, style, bpm_range, era
        )
        
        # Search across platforms
        all_sources = []
        for platform in platforms:
            platform_sources = await self._search_platform(
                platform, queries, max_per_platform
            )
            all_sources.extend(platform_sources)
        
        # Categorize and assess quality if needed
        if kwargs.get("assess_quality"):
            all_sources = await self._assess_quality(all_sources)
        
        # Filter by quality threshold
        if quality_threshold > 0:
            all_sources = [s for s in all_sources 
                         if s.get("quality_score", 1.0) >= quality_threshold]
        
        # Validate URLs if requested
        if kwargs.get("validate_urls"):
            valid_count = 0
            for source in all_sources:
                source["valid"] = validate_url(source["url"])
                if source["valid"]:
                    valid_count += 1
        
        # Estimate sizes if requested
        if kwargs.get("estimate_sizes"):
            total_size = 0
            for source in all_sources:
                size = estimate_download_size(
                    source["url"], 
                    source.get("duration", 180)
                )
                source["estimated_size"] = size
                total_size += size
        
        # Log token usage
        await self._log_token_usage(task_id, 150)  # Mock usage
        
        result_data = {
            "search_queries": queries,
            "sources": all_sources[:max_results],
            "total_found": len(all_sources)
        }
        
        if kwargs.get("validate_urls"):
            result_data["valid_sources"] = valid_count
            result_data["invalid_sources"] = len(all_sources) - valid_count
        
        if kwargs.get("estimate_sizes"):
            result_data["total_estimated_size"] = total_size
        
        return AgentResult(
            agent_name=self.name,
            task_id=task_id,
            status=AgentStatus.SUCCESS,
            result=result_data,
            started_at=started_at,
            completed_at=datetime.now(timezone.utc)
        )
    
    async def _process_existing_sources(
        self,
        task_id: str,
        started_at: datetime,
        **kwargs
    ) -> AgentResult:
        """Process existing sources (categorize, filter, etc.)."""
        sources = kwargs.get("sources", [])
        target_genre = kwargs.get("target_genre")
        filter_criteria = kwargs.get("filter_criteria", {})
        
        processed_sources = sources.copy()
        
        # Categorize if target genre provided
        if target_genre:
            processed_sources = await self._categorize_sources(
                processed_sources, target_genre
            )
            
            # Filter by relevance if threshold exists
            min_relevance = filter_criteria.get("min_quality", 0.5)
            processed_sources = [s for s in processed_sources 
                               if s.get("relevance", 1.0) >= min_relevance]
        
        # Smart filtering if criteria provided
        if filter_criteria:
            max_results = kwargs.get("max_results", len(processed_sources))
            processed_sources = await self._smart_filter(
                processed_sources, filter_criteria, max_results
            )
        
        # Create summary if requested
        summary = None
        if kwargs.get("create_summary"):
            summary = await self._create_collection_summary(processed_sources)
        
        # Log token usage
        await self._log_token_usage(task_id, 100)  # Mock usage
        
        result_data = {
            "sources": processed_sources
        }
        
        if target_genre:
            result_data["categorized_sources"] = processed_sources
        
        if filter_criteria:
            result_data["filtered_sources"] = processed_sources
        
        if summary:
            result_data["collection_summary"] = summary
        
        return AgentResult(
            agent_name=self.name,
            task_id=task_id,
            status=AgentStatus.SUCCESS,
            result=result_data,
            started_at=started_at,
            completed_at=datetime.now(timezone.utc)
        )
    
    async def _generate_search_queries(
        self,
        genre: str,
        style: str,
        bpm_range: Optional[Tuple[int, int]],
        era: str
    ) -> List[str]:
        """Generate optimized search queries for YouTube."""
        queries = []
        
        # Base query with genre and style
        if genre and style:
            base_query = f"{genre} {style}"
        elif genre:
            base_query = genre
        else:
            base_query = "samples"
        
        # Primary queries
        queries.append(f"{base_query} samples")
        queries.append(f"{base_query} drum kit")
        queries.append(f"{base_query} loop pack")
        
        # BPM-specific queries
        if bpm_range:
            bpm_query = f"{base_query} {bpm_range[0]}-{bpm_range[1]} BPM"
            queries.append(bpm_query)
            queries.append(f"{bpm_query} samples")
        
        # Era-specific queries
        if era:
            era_mappings = {
                "60s": ["1960s", "sixties", "vintage"],
                "70s": ["1970s", "seventies", "funk soul"],
                "80s": ["1980s", "eighties", "synth drum machine"],
                "90s": ["1990s", "nineties", "boom bap golden era"],
                "2000s": ["2000s", "y2k", "early 2000s"]
            }
            
            era_terms = era_mappings.get(era.lower(), [era])
            for term in era_terms[:2]:  # Use top 2 era terms
                queries.append(f"{term} {base_query} samples")
        
        # Quality indicators
        queries.append(f"{base_query} samples free download")
        queries.append(f"{base_query} WAV samples")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in queries:
            if q not in seen and q.strip():
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:7]  # Return top 7 queries
    
    async def _search_platform(
        self,
        platform: str,
        queries: List[str],
        max_per_platform: int
    ) -> List[Dict]:
        """Search a specific platform for samples."""
        results = []
        
        for query in queries:
            if platform == "youtube":
                platform_results = await search_youtube(query, max_per_platform)
            elif platform == "soundcloud":
                platform_results = await search_soundcloud(query, max_per_platform)
            elif platform == "freesound":
                platform_results = await search_freesound(query, max_per_platform)
            else:
                continue
            
            results.extend(platform_results)
            
            if len(results) >= max_per_platform:
                break
        
        return results[:max_per_platform]
    
    async def _categorize_sources(
        self,
        sources: List[Dict],
        target_genre: str
    ) -> List[Dict]:
        """Categorize sources using AI."""
        # Mock categorization for testing
        # In production, use actual AI agent
        categorized = []
        
        for source in sources:
            # Simple mock logic
            if "classical" in source.get("title", "").lower():
                relevance = 0.2
                category = "classical"
            elif "hip" in source.get("title", "").lower() and target_genre == "hip-hop":
                relevance = 0.95
                category = "hip-hop"
            else:
                relevance = 0.7
                category = "funk"
            
            categorized_source = source.copy()
            categorized_source.update({
                "category": category,
                "relevance": relevance,
                "reason": f"Categorized based on title analysis"
            })
            categorized.append(categorized_source)
        
        return categorized
    
    async def _assess_quality(self, sources: List[Dict]) -> List[Dict]:
        """Assess quality of sources using AI."""
        assessed = []
        
        for source in sources:
            # Mock quality assessment
            quality_score = 0.9 if "WAV" in source.get("title", "") else 0.7
            
            assessed_source = source.copy()
            assessed_source["quality_score"] = quality_score
            assessed_source["quality_factors"] = {
                "title_indicates_quality": "quality" in source.get("title", "").lower(),
                "professional_description": "professional" in source.get("description", "").lower(),
                "format_mentioned": "WAV" if "WAV" in source.get("title", "") else None
            }
            assessed.append(assessed_source)
        
        return assessed
    
    async def _smart_filter(
        self,
        sources: List[Dict],
        criteria: Dict,
        max_results: int
    ) -> List[Dict]:
        """Apply smart filtering using AI."""
        # Mock filtering for testing
        filtered = []
        
        for i, source in enumerate(sources):
            if i % 2 == 0 and len(filtered) < max_results:  # Simple mock logic
                source["keep"] = True
                source["filter_reason"] = "Quality/variety"
                filtered.append(source)
        
        return filtered
    
    async def _create_collection_summary(
        self,
        sources: List[Dict]
    ) -> Dict[str, Any]:
        """Create AI-generated collection summary."""
        # Mock summary for testing
        categories = set(s.get("category", "unknown") for s in sources)
        
        return {
            "description": f"Diverse collection spanning {', '.join(categories)}",
            "recommended_use": "Perfect for neo-soul and jazz-fusion production",
            "key_characteristics": ["vintage", "organic", "groove-oriented"],
            "total_sources": len(sources)
        }
    
    async def _log_token_usage(self, task_id: str, tokens: int) -> None:
        """Log AI token usage."""
        await database.add_agent_log({
            "task_id": task_id,
            "agent_type": self.name,
            "log_level": "info",
            "message": "AI token usage",
            "context": {
                "tokens_used": tokens,
                "model": "google/gemini-flash-2.0"
            }
        })